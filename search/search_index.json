{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"haiku.skills","text":"<p>Skill-powered AI agents implementing the Agent Skills specification with pydantic-ai.</p>"},{"location":"#how-it-works","title":"How it works","text":"<p><code>SkillToolset</code> is a pydantic-ai <code>FunctionToolset</code> that you attach to your own agent. It exposes a single <code>execute_skill</code> tool. When the agent calls it, a focused sub-agent spins up with only that skill's instructions and tools \u2014 then returns the result. The main agent never sees the skill's internal tools, so its tool space stays clean no matter how many skills you load.</p> <p>This sub-agent architecture means each skill runs in isolation with its own system prompt, tools, and token budget. Skills don't interfere with each other, tool descriptions don't compete for attention, and failures in one skill can't confuse another.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Sub-agent execution \u2014 Each skill runs in its own agent with dedicated instructions and tools</li> <li>Skill discovery \u2014 Scan filesystem paths for SKILL.md directories or load from Python entrypoints</li> <li>In-process tools \u2014 Attach pydantic-ai <code>Tool</code> functions or <code>AbstractToolset</code> instances to skills</li> <li>Per-skill state \u2014 Skills declare a Pydantic state model and namespace; state is passed to tools via <code>RunContext</code> and tracked on the toolset</li> <li>AG-UI protocol \u2014 State changes emit <code>StateDeltaEvent</code> (JSON Patch), compatible with the AG-UI protocol</li> <li>Script tools \u2014 Python, JavaScript, TypeScript, and shell scripts in <code>scripts/</code>; Python scripts with a <code>main()</code> function are AST-parsed for typed tool schemas and executed via <code>uv run</code> with PEP 723 dependency support</li> <li>MCP integration \u2014 Wrap any MCP server (stdio, SSE, streamable HTTP) as a skill</li> </ul>"},{"location":"#quick-install","title":"Quick install","text":"<pre><code>uv add haiku.skills\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation \u2014 Install with optional extras</li> <li>Quick start \u2014 Create a skill and use SkillToolset</li> <li>Skills \u2014 SKILL.md format, tools, state, and script tools</li> <li>Skill sources \u2014 Filesystem, entrypoints, and MCP integration</li> <li>Examples \u2014 Practical examples for each source type</li> <li>CLI \u2014 Command-line interface reference</li> <li>AG-UI protocol \u2014 State deltas and the AG-UI protocol</li> <li>Development \u2014 Contributing, testing, and tooling</li> </ul>"},{"location":"ag-ui/","title":"AG-UI protocol","text":"<p>haiku.skills supports the AG-UI protocol for communicating state changes to frontend clients.</p>"},{"location":"ag-ui/#installation","title":"Installation","text":"<pre><code>uv add \"haiku.skills[ag-ui]\"\n</code></pre>"},{"location":"ag-ui/#per-skill-state","title":"Per-skill state","text":"<p>Skills can declare a Pydantic state model and a namespace. State is passed to tool functions via <code>RunContext[SkillRunDeps]</code> and tracked per namespace on the <code>SkillToolset</code>.</p> <pre><code>from pydantic import BaseModel\nfrom haiku.skills import Skill, SkillMetadata, SkillSource\n\nclass MyState(BaseModel):\n    items: list[str] = []\n\nskill = Skill(\n    metadata=SkillMetadata(name=\"my-skill\", description=\"...\"),\n    source=SkillSource.ENTRYPOINT,\n    instructions=\"...\",\n    state_type=MyState,\n    state_namespace=\"my-skill\",\n)\n</code></pre>"},{"location":"ag-ui/#state-snapshots","title":"State snapshots","text":"<p>The toolset provides methods for working with state:</p> <pre><code>from haiku.skills import SkillToolset\n\ntoolset = SkillToolset(skills=[skill])\n\n# Get a snapshot of all namespaced state\ntoolset.build_state_snapshot()    # {\"my-skill\": {\"items\": []}}\n\n# Restore from a snapshot\ntoolset.restore_state_snapshot({\"my-skill\": {\"items\": [\"hello\"]}})\n\n# Get a specific namespace\ntoolset.get_namespace(\"my-skill\") # MyState(items=[\"hello\"])\n\n# Schema information\ntoolset.state_schemas             # {\"my-skill\": &lt;JSON schema&gt;}\n</code></pre>"},{"location":"ag-ui/#state-schemas","title":"State schemas","text":"<p><code>state_schemas</code> returns the JSON Schema for each namespace, useful for building typed frontend components or validating state:</p> <pre><code>toolset.state_schemas\n# {\n#     \"my-skill\": {\n#         \"properties\": {\n#             \"items\": {\n#                 \"default\": [],\n#                 \"items\": {\"type\": \"string\"},\n#                 \"title\": \"Items\",\n#                 \"type\": \"array\",\n#             }\n#         },\n#         \"title\": \"MyState\",\n#         \"type\": \"object\",\n#     }\n# }\n</code></pre> <p>Schemas are standard JSON Schema generated from the Pydantic state models. Nested models produce <code>$defs</code> references as usual.</p> <p>Note</p> <p>The AG-UI protocol does not currently define a standard mechanism for communicating state schemas to clients. <code>state_schemas</code> is available on the Python side \u2014 how you expose it to frontends (e.g. a dedicated endpoint, initial metadata) is up to your application.</p>"},{"location":"ag-ui/#state-deltas","title":"State deltas","text":"<p>When <code>execute_skill</code> runs a skill whose tools modify state, the toolset computes a JSON Patch delta between the state before and after execution. This delta is returned as a <code>StateDeltaEvent</code>, compatible with the AG-UI protocol.</p> <p>Frontends can apply these patches incrementally to keep their view of the agent's state in sync without polling or full state transfers.</p>"},{"location":"ag-ui/#state-round-tripping","title":"State round-tripping","text":"<p>When serving an agent via AG-UI (using <code>handle_ag_ui_request</code> or <code>AGUIAdapter</code>), the frontend sends state with each request. The adapter injects that state into <code>deps.state</code> if the deps object implements pydantic-ai's <code>StateHandler</code> protocol. <code>SkillToolset</code> then automatically restores per-namespace state from <code>deps.state</code> at the start of each run, closing the loop between frontend and backend.</p> <p>haiku.skills provides <code>SkillDeps</code> \u2014 a minimal dataclass that satisfies pydantic-ai's <code>StateHandler</code> protocol with a <code>dict</code> state matching the namespace snapshot shape that <code>SkillToolset</code> expects:</p> <pre><code>from pydantic_ai import Agent\nfrom pydantic_ai.ag_ui import handle_ag_ui_request\nfrom haiku.skills import SkillDeps, SkillToolset, build_system_prompt\n\ntoolset = SkillToolset(use_entrypoints=True)\nagent = Agent(\n    \"anthropic:claude-sonnet-4-5-20250929\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n    deps_type=SkillDeps,\n)\n\n# In your FastAPI route:\n# return await handle_ag_ui_request(agent, request, deps=SkillDeps())\n</code></pre> <p>Note</p> <p><code>SkillDeps</code> operates at the agent level \u2014 it carries the full AG-UI state dict (all namespaces) and is managed by the adapter. <code>SkillRunDeps</code>, on the other hand, is internal to <code>SkillToolset</code>: when a skill sub-agent runs, it receives <code>SkillRunDeps</code> containing only that skill's per-namespace state model. You don't need to create <code>SkillRunDeps</code> yourself.</p> <p>Custom dependencies</p> <p>If your agent needs additional dependencies beyond state, create your own dataclass with a <code>state: dict[str, Any]</code> field:</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\n@dataclass\nclass MyDeps:\n    state: dict[str, Any] = field(default_factory=dict)\n    db: MyDatabase = ...\n</code></pre> <p>Any dataclass with a <code>state</code> attribute satisfies the <code>StateHandler</code> protocol.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#052-2026-03-02","title":"0.5.2 - 2026-03-02","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Graphiti memory skill (<code>haiku-skills-graphiti-memory</code>): Store, recall, and forget memories using a knowledge graph powered by Graphiti and FalkorDB \u2014 with per-skill state tracking</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li><code>SkillMetadata.allowed_tools</code> accepts strings: Now accepts both <code>str</code> (space-separated) and <code>list[str]</code> as input, always stores <code>list[str]</code> \u2014 eliminates conversion overhead for consumers using the spec's string format (#19)</li> <li><code>Skill.model</code> accepts <code>Model</code> instances: Widened from <code>str | None</code> to <code>str | Model | None</code> so consumers can pass configured model objects directly (#20)</li> <li><code>discover_from_paths</code> accepts single-skill directories: Paths that contain <code>SKILL.md</code> directly are now treated as skill directories, in addition to parent directories containing skill subdirectories. Dot-directories are skipped during child iteration.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Ollama base URL handling: <code>resolve_model()</code> now appends <code>/v1</code> to <code>OLLAMA_BASE_URL</code> instead of expecting it in the env var, consistent with Ollama's convention</li> <li>Web skill <code>fetch_page</code> for non-HTML content: Pages with non-HTML content types (e.g. plain text, markdown) are now returned directly instead of failing with \"could not extract content\"</li> </ul>"},{"location":"changelog/#051-2026-02-27","title":"0.5.1 - 2026-02-27","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li><code>build_system_prompt()</code> utility: Standalone function to build the main agent system prompt from a skill catalog, with optional custom preamble \u2014 replaces <code>SkillToolset.system_prompt</code> property</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Entrypoint skill priority: Skills passed via <code>skills=</code> now take priority over entrypoint-discovered skills \u2014 entrypoints with the same name are silently skipped instead of raising a duplicate error</li> <li>Sub-agent request limit: Increased from 10 to 20 to allow skills with more complex tool chains to complete</li> <li>Chat TUI tool call display: Tool call widgets now stream argument updates and show richer descriptions (e.g. <code>execute_skill \u2192 web: search for ...</code>)</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>SkillToolset.system_prompt</code>: Use <code>build_system_prompt(toolset.skill_catalog)</code> instead</li> </ul>"},{"location":"changelog/#050-2026-02-25","title":"0.5.0 - 2026-02-25","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li><code>skill_model</code> parameter: <code>SkillToolset</code> accepts <code>skill_model</code> to set the model for skill sub-agents (also available as <code>--skill-model</code> CLI option)</li> <li><code>resolve_model()</code>: Resolves model strings with transparent <code>ollama:</code> prefix handling (defaults to <code>http://127.0.0.1:11434</code> when <code>OLLAMA_BASE_URL</code> is unset)</li> <li><code>run_script</code> tool: Skill sub-agents can execute scripts from the skill's <code>scripts/</code> directory via a <code>run_script</code> tool, supporting <code>.py</code>, <code>.sh</code>, <code>.js</code>, <code>.ts</code>, and generic executables with path validation</li> <li>JS/TS script support: <code>run_script</code> dispatches <code>.js</code> files via <code>node</code> and <code>.ts</code> files via <code>npx tsx</code>; extensible via <code>SCRIPT_RUNNERS</code> mapping</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Script tool execution: Scripts are now invoked with CLI positional arguments (<code>sys.argv</code> + <code>print()</code>) instead of JSON on stdin/stdout, matching standard CLI conventions and enabling compatibility with external skill scripts</li> <li>Resilient script discovery: <code>discover_script_tools()</code> now skips scripts without a <code>main()</code> function (with a warning) instead of crashing</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Script failure error reporting: Script error messages now include stdout when stderr is empty, so usage messages and other stdout-based errors are visible to the sub-agent</li> <li>Script sibling imports: <code>run_script</code> and typed script tools now set <code>PYTHONPATH</code> to the skill directory so scripts can use package-style imports (e.g. <code>from scripts.utils import ...</code>)</li> </ul>"},{"location":"changelog/#042-2026-02-20","title":"0.4.2 - 2026-02-20","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li><code>SkillDeps</code>: Minimal dataclass satisfying pydantic-ai's <code>StateHandler</code> protocol for type-correct AG-UI state round-tripping (replaces <code>StateDeps[dict[str, Any]]</code> recommendation in docs)</li> </ul>"},{"location":"changelog/#041-2026-02-20","title":"0.4.1 - 2026-02-20","text":""},{"location":"changelog/#041-2026-02-20_1","title":"0.4.1 - 2026-02-20","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>AG-UI state restoration: <code>SkillToolset</code> now restores skill namespace state from frontend-provided <code>deps.state</code> on each AG-UI request, so state survives server restarts</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>RAG skill package (<code>haiku-skills-rag</code>): Moved to haiku.rag</li> </ul>"},{"location":"changelog/#040-2026-02-19","title":"0.4.0 - 2026-02-19","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li><code>haiku-skills validate</code> command: Validate skill directories against the Agent Skills specification using <code>skills-ref</code></li> <li>Unknown frontmatter rejection: <code>SkillMetadata</code> now rejects unknown fields (<code>extra=\"forbid\"</code>)</li> <li><code>skills-ref</code> dependency: Reference implementation used for spec-compliant validation</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Distributable skill directory layout: SKILL.md moved into a subdirectory matching the skill name (e.g. <code>haiku_skills_web/web/SKILL.md</code>) so all bundled skills pass directory-name validation</li> </ul>"},{"location":"changelog/#030-2026-02-19","title":"0.3.0 - 2026-02-19","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li><code>haiku-skills list</code> command: List discovered skills with name and description, supports <code>-s</code>/<code>--skill-path</code> and <code>--use-entrypoints</code></li> <li><code>--skill</code> / <code>-k</code> option for <code>chat</code>: Filter which skills to activate by name (repeatable)</li> <li>RAG skill package (<code>haiku-skills-rag</code>): Search, retrieve and analyze documents via haiku.rag with tools for hybrid search, document listing/retrieval, QA with citations, and code-execution analysis</li> <li>Web skill package (<code>haiku-skills-web</code>): Web search via Brave Search API and page content extraction via trafilatura (replaces <code>haiku-skills-brave-search</code>)</li> <li>Per-skill state: Skills can declare a <code>state_type</code> (Pydantic <code>BaseModel</code>) and <code>state_namespace</code>; state is passed to tool functions via <code>RunContext[SkillRunDeps]</code> and tracked per namespace on the toolset</li> <li>AG-UI protocol: <code>SkillToolset</code> emits <code>StateDeltaEvent</code> (JSON Patch) when skill execution changes state, compatible with the AG-UI protocol</li> <li>State API on <code>SkillToolset</code>: <code>build_state_snapshot()</code>, <code>restore_state_snapshot()</code>, <code>get_namespace()</code>, <code>state_schemas</code></li> <li>In-process tools with state: Distributable skills (web, image-generation, code-execution, rag) converted from script-based to in-process tool functions that can read and write per-skill state</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Skills fully loaded at discovery: Instructions, script tools, and resources are loaded when skills are discovered, removing the separate activation step</li> <li>Chat TUI rewritten as AG-UI client: Uses <code>AGUIAdapter</code> event stream instead of polling; inline state delta display and a \"View state\" modal via the command palette</li> <li>Skill name validation: Now accepts unicode lowercase alphanumeric characters per the Agent Skills specification (previously ASCII-only)</li> <li>Documentation site: Published at ggozad.github.io/haiku.skills with MkDocs Material</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Brave Search skill package (<code>haiku-skills-brave-search</code>): Replaced by <code>haiku-skills-web</code></li> <li><code>SkillRegistry.activate()</code>: Skills are fully loaded at discovery time; progressive disclosure removed</li> <li><code>Task</code> / <code>TaskStatus</code>: Task tracking removed from <code>SkillToolset</code>; the AG-UI adapter provides tool call progress via events</li> </ul>"},{"location":"changelog/#010-2026-02-16","title":"0.1.0 - 2026-02-16","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Core framework: Skill-powered AI agents implementing the Agent Skills specification with pydantic-ai</li> <li>Skill model: Pydantic v2 models for skills, metadata, and tasks with full validation</li> <li>SKILL.md parser: YAML frontmatter + markdown body parsing following the Agent Skills spec</li> <li>Skill discovery: Filesystem scanning (directories containing SKILL.md) and Python entrypoint-based plugin discovery</li> <li>SkillRegistry: Central registry for skill discovery, loading, lookup, and activation</li> <li>Progressive disclosure: Three-level progressive disclosure \u2014 metadata at startup, instructions on activation, resources on demand</li> <li>Sub-agent delegation: Each skill runs in a focused sub-agent with its own system prompt and tools via <code>execute_skill</code></li> <li>SkillToolset: <code>FunctionToolset</code> integration that exposes skills as tools for any pydantic-ai <code>Agent</code></li> <li>Script tools: Python scripts in <code>scripts/</code> with <code>main()</code> function get AST-parsed into typed pydantic-ai <code>Tool</code> objects with automatic parameter schema extraction</li> <li>Resource reading: Skills can expose files (references, assets, templates) as resources; sub-agents read them on demand via <code>read_resource</code> tool with path validation and traversal defense</li> <li>MCP integration: <code>skill_from_mcp()</code> maps MCP servers directly to skills</li> <li>Chat TUI: Terminal-based chat interface using Textual</li> <li>Distributable skill packages: Workspace members for brave-search, image-generation, and code-execution skills</li> </ul>"},{"location":"cli/","title":"CLI","text":"<p>haiku.skills provides a <code>haiku-skills</code> command-line interface.</p>"},{"location":"cli/#validate","title":"<code>validate</code>","text":"<p>Validate skill directories against the Agent Skills specification using the reference implementation:</p> <pre><code>haiku-skills validate ./skills/web ./skills/calculator\n</code></pre> <p>Prints <code>VALID</code> or <code>INVALID</code> with error details for each path. Exits with code 1 if any skill is invalid.</p>"},{"location":"cli/#list","title":"<code>list</code>","text":"<p>List discovered skills with name and description:</p> <pre><code># From filesystem paths\nhaiku-skills list -s ./skills\n\n# From entrypoints\nhaiku-skills list --use-entrypoints\n\n# Both\nhaiku-skills list -s ./skills --use-entrypoints\n</code></pre>"},{"location":"cli/#chat","title":"<code>chat</code>","text":"<p>A debug/development chat TUI built with Textual. Requires the <code>tui</code> extra:</p> <pre><code>uv add \"haiku.skills[tui]\"\n</code></pre> <p>Point it at a directory of skills for filesystem discovery:</p> <pre><code>haiku-skills chat -s ./skills -m openai:gpt-4o\n</code></pre> <p>Or use entrypoint discovery:</p> <pre><code>haiku-skills chat --use-entrypoints -m openai:gpt-4o\n</code></pre> <p>Filter to specific skills by name:</p> <pre><code>haiku-skills chat --use-entrypoints -k web -k code-execution -m openai:gpt-4o\n</code></pre> <p>Set the model to use for skill sub-agents (overrides <code>HAIKU_SKILL_MODEL</code> env var):</p> <pre><code>haiku-skills chat -s ./skills -m openai:gpt-4o --skill-model ollama:llama3\n</code></pre> <p>The <code>tui</code> extra includes <code>ag-ui-protocol</code>. The chat TUI uses the AG-UI protocol adapter for event streaming, making it useful for debugging skills with per-skill state:</p> <ul> <li>State deltas are displayed inline as JSON Patch operations whenever a skill modifies state</li> <li>Full state snapshot is available via the \"View state\" modal in the command palette</li> </ul>"},{"location":"cli/#environment-variables","title":"Environment variables","text":"Variable Description <code>HAIKU_SKILLS_MODEL</code> Default main agent model for <code>chat</code> (fallback when <code>-m</code> is not provided, defaults to <code>ollama:gpt-oss</code>) <code>HAIKU_SKILL_MODEL</code> Model to use for skill sub-agents (overridden by <code>--skill-model</code> or per-skill <code>model</code> in SKILL.md) <code>HAIKU_SKILLS_PATHS</code> Colon-separated skill directory paths (fallback when <code>-s</code> is not provided) <code>HAIKU_SKILLS_USE_ENTRYPOINTS</code> Set to <code>1</code>, <code>true</code>, or <code>yes</code> to enable entrypoint discovery by default"},{"location":"development/","title":"Development","text":""},{"location":"development/#setup","title":"Setup","text":"<pre><code>git clone https://github.com/ggozad/haiku.skills.git\ncd haiku.skills\nuv sync --all-extras\n</code></pre>"},{"location":"development/#running-tests","title":"Running tests","text":"<pre><code>uv run pytest\n</code></pre> <p>With coverage (100% is required):</p> <pre><code>uv run pytest --cov\n</code></pre>"},{"location":"development/#vcr-integration-tests","title":"VCR integration tests","text":"<p>Integration tests are recorded against Ollama and replayed from cassettes:</p> <pre><code># Run from cassettes (default)\nuv run pytest tests/test_integration.py\n\n# Record new cassettes\nuv run pytest tests/test_integration.py --record-mode=new_episodes\n</code></pre>"},{"location":"development/#linting-and-formatting","title":"Linting and formatting","text":"<pre><code>uv run ruff check\nuv run ruff format --check\n</code></pre>"},{"location":"development/#type-checking","title":"Type checking","text":"<pre><code>uv run ty check\n</code></pre>"},{"location":"development/#building-docs","title":"Building docs","text":"<pre><code>uv run mkdocs build --strict\nuv run mkdocs serve  # Preview at http://127.0.0.1:8000\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#filesystem-skill","title":"Filesystem skill","text":"<p>A complete skill directory with a <code>SKILL.md</code>, script tool, and resource file:</p> <pre><code>my-skill/\n\u251c\u2500\u2500 SKILL.md\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 analyze.py\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 reference.txt\n</code></pre> <p><code>SKILL.md</code>:</p> <pre><code>---\nname: my-skill\ndescription: Analyze data using reference material.\nresources:\n  - data/reference.txt\n---\n\n# My Skill\n\nYou help users analyze data. Use the `analyze` script tool to process\ninput, and read the reference resource when you need context.\n</code></pre> <p><code>scripts/analyze.py</code>:</p> <pre><code># /// script\n# dependencies = [\"pandas\"]\n# ///\n\"\"\"Analyze data.\"\"\"\nimport sys\n\nimport pandas as pd\n\ndef main(data: str, operation: str = \"describe\") -&gt; str:\n    \"\"\"Analyze the given data.\n\n    Args:\n        data: Input data to analyze.\n        operation: Analysis operation to perform.\n    \"\"\"\n    df = pd.read_csv(pd.io.common.StringIO(data))\n    if operation == \"describe\":\n        return df.describe().to_string()\n    return f\"Analyzed {len(df)} rows\"\n\nif __name__ == \"__main__\":\n    data = sys.argv[1]\n    operation = sys.argv[2] if len(sys.argv) &gt; 2 else \"describe\"\n    print(main(data, operation))\n</code></pre> <p>Usage:</p> <pre><code>from pathlib import Path\nfrom pydantic_ai import Agent\nfrom haiku.skills import SkillToolset, build_system_prompt\n\ntoolset = SkillToolset(skill_paths=[Path(\"./my-skill\")])\nagent = Agent(\n    \"anthropic:claude-sonnet-4-5-20250929\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n</code></pre>"},{"location":"examples/#entrypoint-skill","title":"Entrypoint skill","text":"<p>A minimal package that registers a skill with tools and state via entrypoints.</p> <p><code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"my-skill-package\"\nversion = \"0.1.0\"\ndependencies = [\"haiku.skills\"]\n\n[project.entry-points.\"haiku.skills\"]\ncalculator = \"my_skill_package:create_skill\"\n</code></pre> <p><code>my_skill_package/__init__.py</code>:</p> <pre><code>from pydantic import BaseModel\nfrom pydantic_ai import RunContext\nfrom haiku.skills import Skill, SkillMetadata, SkillSource\nfrom haiku.skills.state import SkillRunDeps\n\nclass CalculatorState(BaseModel):\n    history: list[str] = []\n\ndef add(ctx: RunContext[SkillRunDeps], a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers.\"\"\"\n    result = a + b\n    if ctx.deps and ctx.deps.state and isinstance(ctx.deps.state, CalculatorState):\n        ctx.deps.state.history.append(f\"{a} + {b} = {result}\")\n    return result\n\ndef create_skill() -&gt; Skill:\n    return Skill(\n        metadata=SkillMetadata(\n            name=\"calculator\",\n            description=\"Perform mathematical calculations.\",\n        ),\n        source=SkillSource.ENTRYPOINT,\n        instructions=\"Use the add tool to add numbers.\",\n        tools=[add],\n        state_type=CalculatorState,\n        state_namespace=\"calculator\",\n    )\n</code></pre> <p>Usage:</p> <pre><code>from haiku.skills import SkillToolset\n\ntoolset = SkillToolset(use_entrypoints=True)\n</code></pre>"},{"location":"examples/#mcp-skill","title":"MCP skill","text":"<p>Wrapping an existing MCP server as a skill:</p> <pre><code>from pydantic_ai.mcp import MCPServerStdio\nfrom pydantic_ai import Agent\nfrom haiku.skills import SkillToolset, build_system_prompt, skill_from_mcp\n\nskill = skill_from_mcp(\n    MCPServerStdio(\"uvx\", args=[\"my-mcp-server\"]),\n    name=\"my-mcp-skill\",\n    description=\"Tools from my MCP server.\",\n    instructions=\"Use these tools when the user asks about...\",\n)\n\ntoolset = SkillToolset(skills=[skill])\nagent = Agent(\n    \"anthropic:claude-sonnet-4-5-20250929\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n</code></pre>"},{"location":"examples/#built-in-skill-packages","title":"Built-in skill packages","text":"<p>haiku.skills includes distributable skill packages under <code>skills/</code>. Each is a standalone Python package that registers itself via entrypoints.</p>"},{"location":"examples/#web","title":"web","text":"<p>Web search via Brave Search API and page content extraction via trafilatura.</p> <pre><code>uv add haiku-skills-web\n</code></pre> <p>Requires <code>BRAVE_API_KEY</code> environment variable for search functionality.</p>"},{"location":"examples/#image-generation","title":"image-generation","text":"<p>Image generation via Ollama.</p> <pre><code>uv add haiku-skills-image-generation\n</code></pre>"},{"location":"examples/#code-execution","title":"code-execution","text":"<p>Sandboxed Python execution via pydantic-monty.</p> <pre><code>uv add haiku-skills-code-execution\n</code></pre>"},{"location":"examples/#graphiti-memory","title":"graphiti-memory","text":"<p>Knowledge graph memory using Graphiti and FalkorDB. Store, recall, and forget facts across conversations.</p> <pre><code>uv add haiku-skills-graphiti-memory\n</code></pre> <p>Requires a running FalkorDB instance. Configure via environment variables:</p> <ul> <li><code>FALKORDB_URI</code> \u2014 FalkorDB connection URI (default: <code>falkor://localhost:6379</code>)</li> <li><code>OLLAMA_BASE_URL</code> \u2014 Ollama API base URL (default: <code>http://localhost:11434</code>)</li> <li><code>GRAPHITI_LLM_MODEL</code> \u2014 LLM model for graph operations (default: <code>gpt-oss</code>)</li> <li><code>GRAPHITI_EMBEDDING_MODEL</code> \u2014 Embedding model (default: <code>qwen3-embedding:4b</code>)</li> <li><code>GRAPHITI_EMBEDDING_DIM</code> \u2014 Embedding dimension (default: <code>2560</code>)</li> <li><code>GRAPHITI_GROUP_ID</code> \u2014 Graph partition identifier (default: <code>default</code>)</li> </ul> <p>For a more elaborate use case involving RAG (retrieval-augmented generation), see the haiku.rag project which provides a full RAG skill for haiku.skills.</p>"},{"location":"examples/#mixing-sources","title":"Mixing sources","text":"<p>Combine filesystem, entrypoint, and MCP skills in a single toolset:</p> <pre><code>from pathlib import Path\nfrom pydantic_ai.mcp import MCPServerStdio\nfrom pydantic_ai import Agent\nfrom haiku.skills import SkillToolset, build_system_prompt, skill_from_mcp\n\nmcp_skill = skill_from_mcp(\n    MCPServerStdio(\"uvx\", args=[\"my-mcp-server\"]),\n    name=\"my-mcp-skill\",\n    description=\"Tools from my MCP server.\",\n)\n\ntoolset = SkillToolset(\n    skill_paths=[Path(\"./skills\")],   # Filesystem skills\n    use_entrypoints=True,              # Entrypoint skills\n    skills=[mcp_skill],                # MCP skills\n    skill_model=\"openai:gpt-4o-mini\",   # Model for skill sub-agents\n)\n\nagent = Agent(\n    \"anthropic:claude-sonnet-4-5-20250929\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#basic","title":"Basic","text":"<pre><code>uv add haiku.skills\n</code></pre> <p>Or with pip:</p> <pre><code>pip install haiku.skills\n</code></pre>"},{"location":"installation/#extras","title":"Extras","text":""},{"location":"installation/#ag-ui-protocol-support","title":"AG-UI protocol support","text":"<p>For AG-UI protocol compatibility (state deltas via JSON Patch):</p> <pre><code>uv add \"haiku.skills[ag-ui]\"\n</code></pre>"},{"location":"installation/#chat-tui","title":"Chat TUI","text":"<p>A debug/development chat interface built with Textual:</p> <pre><code>uv add \"haiku.skills[tui]\"\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>pydantic-ai (installed automatically)</li> </ul>"},{"location":"quickstart/","title":"Quick start","text":""},{"location":"quickstart/#creating-a-skill","title":"Creating a skill","text":"<p>A skill is a directory containing a <code>SKILL.md</code> file with YAML frontmatter:</p> <pre><code>---\nname: my-skill\ndescription: Helps with data analysis tasks.\n---\n\n# My Skill\n\nInstructions for the agent go here...\n</code></pre> <p>See the Agent Skills specification for the full format.</p>"},{"location":"quickstart/#using-skilltoolset","title":"Using SkillToolset","text":"<pre><code>from pathlib import Path\nfrom pydantic_ai import Agent\nfrom haiku.skills import SkillToolset, build_system_prompt\n\ntoolset = SkillToolset(\n    skill_paths=[Path(\"./skills\")],\n    skill_model=\"openai:gpt-4o-mini\",   # optional: model to use for skill sub-agents\n)\nagent = Agent(\n    \"anthropic:claude-sonnet-4-5-20250929\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n\nresult = await agent.run(\"Analyze this dataset.\")\nprint(result.output)\n</code></pre> <p><code>SkillToolset</code> discovers skills from the given paths and exposes a single <code>execute_skill</code> tool. <code>build_system_prompt</code> generates a system prompt listing the available skills. When the agent decides to use a skill, a focused sub-agent handles the request with that skill's instructions and tools.</p>"},{"location":"skill-sources/","title":"Skill sources","text":"<p>Skills can come from three sources: filesystem directories, Python entrypoints, and MCP servers. You can combine all three in a single <code>SkillToolset</code>:</p> <pre><code>from pathlib import Path\nfrom pydantic_ai.mcp import MCPServerStdio\nfrom haiku.skills import SkillToolset, skill_from_mcp\n\nmcp_skill = skill_from_mcp(\n    MCPServerStdio(\"uvx\", args=[\"my-mcp-server\"]),\n    name=\"my-mcp-skill\",\n    description=\"Tools from my MCP server.\",\n)\n\ntoolset = SkillToolset(\n    skill_paths=[Path(\"./skills\")],   # Filesystem\n    use_entrypoints=True,              # Entrypoints\n    skills=[mcp_skill],                # MCP (or any programmatic skill)\n)\n</code></pre>"},{"location":"skill-sources/#filesystem","title":"Filesystem","text":"<p>A filesystem skill is a directory containing a <code>SKILL.md</code> file. <code>SkillToolset</code> scans the paths you provide and discovers all skill directories:</p> <pre><code>from pathlib import Path\nfrom haiku.skills import SkillToolset\n\ntoolset = SkillToolset(skill_paths=[Path(\"./skills\")])\n</code></pre> <p>Each path can be either a parent directory (all immediate subdirectories containing <code>SKILL.md</code> are discovered) or a skill directory itself (a directory that directly contains <code>SKILL.md</code>). The directory name must match the skill name in the frontmatter.</p> <p>Filesystem skills automatically pick up:</p> <ul> <li>Script tools \u2014 Python scripts in a <code>scripts/</code> subdirectory (see Skills \u2014 Script tools)</li> <li>Resources \u2014 Files listed in the <code>resources</code> frontmatter field (see Skills \u2014 Resources)</li> </ul>"},{"location":"skill-sources/#entrypoints","title":"Entrypoints","text":"<p>Packages can expose skills via Python entrypoints, enabling automatic discovery without filesystem paths.</p>"},{"location":"skill-sources/#declaring-entrypoints","title":"Declaring entrypoints","text":"<p>Add an entry to the <code>haiku.skills</code> entrypoint group in your <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"haiku.skills\"]\nmy-skill = \"my_package.skills:create_my_skill\"\n</code></pre> <p>The entrypoint must point to a callable that returns a <code>Skill</code>:</p> <pre><code>from haiku.skills import Skill, SkillMetadata, SkillSource\n\ndef create_my_skill() -&gt; Skill:\n    return Skill(\n        metadata=SkillMetadata(name=\"my-skill\", description=\"Data analysis.\"),\n        source=SkillSource.ENTRYPOINT,\n        instructions=\"# My Skill\\n\\nInstructions here...\",\n    )\n</code></pre>"},{"location":"skill-sources/#discovering-entrypoint-skills","title":"Discovering entrypoint skills","text":"<p>Enable entrypoint discovery when creating a <code>SkillToolset</code>:</p> <pre><code>from haiku.skills import SkillToolset\n\ntoolset = SkillToolset(use_entrypoints=True)\n</code></pre> <p>The CLI also supports entrypoint discovery:</p> <pre><code>haiku-skills list --use-entrypoints\nhaiku-skills chat --use-entrypoints -m openai:gpt-4o\n</code></pre>"},{"location":"skill-sources/#priority","title":"Priority","text":"<p>Skills passed via <code>skills=</code> take priority over entrypoint-discovered skills. If a manually provided skill has the same name as an entrypoint skill, the entrypoint is silently skipped. This lets you override an entrypoint skill with a custom configuration:</p> <pre><code>from haiku.skills import SkillToolset\n\ncustom_skill = create_my_skill(db_path=\"/custom/path\")\ntoolset = SkillToolset(\n    skills=[custom_skill],\n    use_entrypoints=True,  # entrypoint for \"my-skill\" is skipped\n)\n</code></pre>"},{"location":"skill-sources/#mcp","title":"MCP","text":"<p>Any MCP server can be wrapped as a skill using <code>skill_from_mcp</code>.</p>"},{"location":"skill-sources/#stdio-servers","title":"Stdio servers","text":"<pre><code>from pydantic_ai.mcp import MCPServerStdio\nfrom haiku.skills import skill_from_mcp\n\nskill = skill_from_mcp(\n    MCPServerStdio(\"uvx\", args=[\"my-mcp-server\"]),\n    name=\"my-mcp-skill\",\n    description=\"Tools from my MCP server.\",\n    instructions=\"Use these tools when the user asks about...\",\n)\n</code></pre>"},{"location":"skill-sources/#sse-and-streamable-http-servers","title":"SSE and streamable HTTP servers","text":"<pre><code>from pydantic_ai.mcp import MCPServerSSE, MCPServerStreamableHTTP\n\n# SSE\nskill = skill_from_mcp(\n    MCPServerSSE(\"http://localhost:8080/sse\"),\n    name=\"sse-skill\",\n    description=\"Tools via SSE.\",\n)\n\n# Streamable HTTP\nskill = skill_from_mcp(\n    MCPServerStreamableHTTP(\"http://localhost:8080/mcp\"),\n    name=\"http-skill\",\n    description=\"Tools via streamable HTTP.\",\n)\n</code></pre>"},{"location":"skill-sources/#how-it-works","title":"How it works","text":"<p><code>skill_from_mcp</code> creates a <code>Skill</code> with the MCP server's toolset attached. When the skill is executed, the sub-agent connects to the MCP server and uses its tools directly. The MCP server's tools are only visible to the sub-agent \u2014 the main agent only sees the <code>execute_skill</code> tool.</p>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#skillmd-format","title":"SKILL.md format","text":"<p>Each skill is defined by a <code>SKILL.md</code> file following the Agent Skills specification. The file uses YAML frontmatter for metadata and markdown for instructions:</p> <pre><code>---\nname: my-skill\ndescription: A brief description of what the skill does.\n---\n\n# My Skill\n\nDetailed instructions for the sub-agent go here. This content becomes\nthe system prompt when the skill is executed.\n</code></pre> <p>The frontmatter supports fields from the Agent Skills spec: <code>name</code>, <code>description</code>, <code>license</code>, <code>compatibility</code>, <code>metadata</code>, and <code>allowed-tools</code>. Unknown fields are rejected.</p> <p>You can validate a skill directory against the spec with:</p> <pre><code>haiku-skills validate ./my-skill\n</code></pre>"},{"location":"skills/#in-process-tools","title":"In-process tools","text":"<p>Skills can carry tool functions that run in the same process. These are plain Python callables or pydantic-ai <code>Tool</code> objects:</p> <pre><code>from haiku.skills import Skill, SkillMetadata, SkillSource, SkillToolset, build_system_prompt\nfrom pydantic_ai import Agent\n\ndef add(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\nskill = Skill(\n    metadata=SkillMetadata(\n        name=\"calculator\",\n        description=\"Perform mathematical calculations.\",\n    ),\n    source=SkillSource.ENTRYPOINT,\n    instructions=\"Use the add tool to add numbers.\",\n    tools=[add],\n)\n\ntoolset = SkillToolset(skills=[skill])\nagent = Agent(\n    \"anthropic:claude-sonnet-4-5-20250929\",\n    instructions=build_system_prompt(toolset.skill_catalog),\n    toolsets=[toolset],\n)\n</code></pre>"},{"location":"skills/#toolsets","title":"Toolsets","text":"<p>For <code>AbstractToolset</code> instances (e.g. MCP toolsets), use the <code>toolsets</code> parameter instead of <code>tools</code>. See Skill sources for MCP integration details.</p>"},{"location":"skills/#script-tools","title":"Script tools","text":"<p>Skills can include executable scripts in a <code>scripts/</code> directory. Python scripts that define a <code>main()</code> function with type-annotated parameters get AST-parsed into typed tools:</p> <pre><code># /// script\n# dependencies = [\"pandas\"]\n# ///\n\"\"\"Analyze data.\"\"\"\nimport sys\n\nimport pandas as pd\n\ndef main(data: str, operation: str = \"describe\") -&gt; str:\n    \"\"\"Analyze the given data.\n\n    Args:\n        data: Input data to analyze.\n        operation: Analysis operation to perform.\n    \"\"\"\n    df = pd.read_csv(pd.io.common.StringIO(data))\n    if operation == \"describe\":\n        return df.describe().to_string()\n    return f\"Analyzed {len(df)} rows\"\n\nif __name__ == \"__main__\":\n    data = sys.argv[1]\n    operation = sys.argv[2] if len(sys.argv) &gt; 2 else \"describe\"\n    print(main(data, operation))\n</code></pre> <p>Script tools are automatically discovered on skill loading. Scripts with a <code>main()</code> function get AST-parsed into typed pydantic-ai <code>Tool</code> objects with automatic parameter schema extraction. Scripts without <code>main()</code> are skipped (with a warning) during typed tool discovery.</p> <p>Additionally, when a skill has a <code>scripts/</code> directory, the sub-agent receives a <code>run_script</code> tool that can execute any script (<code>.py</code>, <code>.sh</code>, <code>.js</code>, <code>.ts</code>, or generic executable) with free-form arguments. This allows the LLM to invoke scripts that don't follow the <code>main()</code> convention.</p> <p>Typed script tools are executed via <code>uv run</code>, so PEP 723 inline dependency metadata (the <code># /// script</code> block above) is supported \u2014 dependencies are installed automatically.</p>"},{"location":"skills/#script-resolution","title":"Script resolution","text":"<p>The <code>run_script</code> tool expects a relative path under <code>scripts/</code> (e.g. <code>scripts/extract.py</code>). Paths that escape the <code>scripts/</code> directory are rejected. The execution method depends on the file extension:</p> Extension Executor <code>.py</code> Current Python interpreter (<code>sys.executable</code>) <code>.sh</code> <code>bash</code> <code>.js</code> <code>node</code> <code>.ts</code> <code>npx tsx</code> Other Run as executable directly <p>Both typed script tools and <code>run_script</code> prepend the skill directory to <code>PYTHONPATH</code>, so scripts can use package-style sibling imports:</p> <pre><code># scripts/utils.py\ndef helper():\n    return \"shared logic\"\n\n# scripts/main_script.py\nfrom scripts.utils import helper\n</code></pre>"},{"location":"skills/#resources","title":"Resources","text":"<p>Skills can expose files (references, assets, templates) as resources. Sub-agents can read them on demand via the <code>read_resource</code> tool with path validation and traversal defense.</p>"},{"location":"skills/#per-skill-state","title":"Per-skill state","text":"<p>Skills can declare a Pydantic state model. State is passed to tool functions via <code>RunContext[SkillRunDeps]</code> and tracked per namespace on the toolset:</p> <pre><code>from pydantic import BaseModel\nfrom pydantic_ai import RunContext\nfrom haiku.skills import Skill, SkillMetadata, SkillSource, SkillToolset\nfrom haiku.skills.state import SkillRunDeps\n\nclass CalculatorState(BaseModel):\n    history: list[str] = []\n\ndef add(ctx: RunContext[SkillRunDeps], a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers.\"\"\"\n    result = a + b\n    if ctx.deps and ctx.deps.state and isinstance(ctx.deps.state, CalculatorState):\n        ctx.deps.state.history.append(f\"{a} + {b} = {result}\")\n    return result\n\nskill = Skill(\n    metadata=SkillMetadata(\n        name=\"calculator\",\n        description=\"Perform mathematical calculations.\",\n    ),\n    source=SkillSource.ENTRYPOINT,\n    instructions=\"Use the add tool to add numbers.\",\n    tools=[add],\n    state_type=CalculatorState,\n    state_namespace=\"calculator\",\n)\n\ntoolset = SkillToolset(skills=[skill])\n\n# State is accessible via the toolset\nprint(toolset.build_state_snapshot())  # {\"calculator\": {\"history\": []}}\n</code></pre> <p>When <code>execute_skill</code> runs a skill whose tools modify state, the toolset computes a JSON Patch delta and returns it as a <code>StateDeltaEvent</code> \u2014 compatible with the AG-UI protocol. See AG-UI protocol for details.</p>"}]}